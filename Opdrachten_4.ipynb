{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opdrachten bij de Introductie tot de programmeertaal Python -- deel 4: \"Machine Learning\"\n",
    "\n",
    "De opdrachten in dit notebook horen bij het vierde deel van de cursus, waarin we Python gebruiken voor machine learning. De meeste van deze opdrachten komen al op de cursusavond aan bod. Ik raad je aan om alle opdrachten waar je niet aan toe komt tijdens de cursus thuis te doen. In sommige gevallen zit er lesmateriaal in wat anders dan in de opdracht niet langskomt in de lesstof.\n",
    "\n",
    "Alle opdrachten hebben een voorbeeld van een uitwerking die wordt geladen wanneer de cellen met \"%load XXX\" worden uitgevoerd. Let wel:\n",
    "- Probeer het eerst zelf! Je leert er pas echt van door te proberen, eventueel te falen en nog een keer te proberen!\n",
    "- Dit is *een* uitwerking, meerdere varianten zijn waarschijnlijk mogelijk. Overleg bij twijfel gerust met een medecursist, of met de docent!\n",
    "\n",
    "## Keuzeopdrachten!\n",
    "Elke opdrachtnummer komt meerdere keuzes, a, b, c etc. Tijdens de cursus is e slechts tijd genoeg voor 1 van deze, maar kies vooral zelf welke je het meest aanspreekt! Thuis kun je de rest altijd nog eens rustig bekijken.\n",
    "\n",
    "## 1. Supervised machine learning\n",
    "\n",
    "Supervised machine learning kent een heel scala aan technieken. In de instructie is al gekeken naar de logistische regressie, een methode om te classificeren. Hiervoor zijn nog meer methoden. Hier kijken we in opdracht 1a naar de beslisbomen, ook wel bekend onder naam decision tree. De dataset over passagiers van de de titanic is voor dit voorbeeld een bekende kandidaat. Deze zit in scikit-lear en we zullen die ook hier gebruiken.\n",
    "\n",
    "Opgave 1b gaat over lineaire regressie en nearest neighbor regressie. Dit zijn methoden om continue variabelen als target value te \"leren\" (bijvoorbeeld: schat lichaamsgewicht als je de lichaamslengte weet). Hiervoor wordt een kunstmatige dataset gebruikt die we maken met functionaliteiten uit numpy.\n",
    "\n",
    "\n",
    "### 1a. Wie overleefde de ramp met de titanic?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Leren van je buren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Unsupervised machine learning\n",
    "\n",
    "We hebben in de instructie gekeken naar k-means clustering. In de klasse van clusteringalgoritmen kijken we in opgave 2a nog naar niet-sferische clusters met k-means en hoe je probelemen daarmee kunt voorkomen door \"density-based\" en \"hierarchische\" clustermethoden.\n",
    "\n",
    "Opgave 2b gaat over de andere klasse van unsupervised learning: dimensiereductie en patroonherkenning. We gebruiken hiervoor Principal Component Analysis, IsoMap en t-SNE. De datasets zullen voor je worden gegenereerd en gevisualiseerd.\n",
    "\n",
    "### 2a. Clustering vervolgd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neurale netwerken\n",
    "\n",
    "Voor het stuk over neurale netwerken zuellen we verder gaan met de MNIST hand-written digits. Opnieuw zijn er twee keuze opdrachten. In 3a. kijken waar naar het gebruik van bottleneck-layers voor dimensiereductie. Een toepassing van auto-encoders als de-noisers (ruisverwijdering uit plaatjes) is het onderwerp van 3b.\n",
    "\n",
    "### 3a. Dimensiereductie met neurale netwerken.\n",
    "Een auto-encoder reproduceert de input-laag, maar het is inzichtelijk (en potentieel heel nuttig!) om ook een netwerk met een bottleneck de labels te laten voorspellen. Hieronder proberen we dat voor een bottleneck met slechts twee neuronen.\n",
    "\n",
    "De data wordt eerst voor je ingeladen, daarna is het aan jou om een netwerk met een bottleneck (met twee neuronen) te trainen. Let op dat je neurale netwerk wel weer grotere lagen krijgt na de bottleneck.\n",
    "\n",
    "Gebruik daarna ook de \"summary\" methode om te kijken of je netwerk eruit ziet zoals je verwacht.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mnist = fetch_mldata(\"MNIST original\", data_home='./data/')  \n",
    "features, labels = mnist.data / 255., mnist.target\n",
    "\n",
    "# Het splitsen in een training en test set gebeurt hier. Check de documentatie!\n",
    "xtr, x, ytr, y = train_test_split(features, labels, test_size=0.3)\n",
    "\n",
    "print(\"Dimensies van xtr:\", xtr.shape)\n",
    "print(\"Dimensies van ytr:\", ytr.shape)\n",
    "print(\"Dimensies van x:\", x.shape)\n",
    "print(\"Dimensies van y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_include = os.path.join('uitwerkingen', '4-nn_dimensiereductie.py') \n",
    "# %load $to_include"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ik wil hier benadrukken dat 95% van de labels correct is voorspeld (in het voorbeeld in de uitwerking), terwijl er een bottleneck-laag met slechts twee neuronen in het netwerk zit! Die laag heeft slechts 2 outputwaarden, waarna het weer opbouwende netwerk de labels bijna helemaal goed kan voorspellen. *Alle info om de labels te voorspellen ligt dus besloten in die 2 getallen!* \n",
    "\n",
    "Bedenk waar dat voor gebruikt kan worden. Door de plaatjes te encoden met de eerste helft van het netwerk (t/m bottleneck) kun je de plaatjes reduceren tot twee getallen. Door het tweede deel van het newterk, de decoder, te gebruiken op deze twee getallen kun je de labels behoorlijk goed reproduceren. \n",
    "\n",
    "Aangezien de bottleneck-laag tweedimensionaal is, kun je deze ook goed visualiseren. Het zou hierin evident moeten zijn dat er tien verschillende labels zijn. Er wordt gebruik gemaakt van de backend van keras, die een functie kan definieren die de waarden van verschillende layers in je netwerk naar elkaar mapt (feitelijk maak je hier dus de encoder). Definieer zo'n functie met behulp van tensorflow.keras.backend.function(). Zie de documentatie voor hoe dit werkt.\n",
    "\n",
    "Plot alle punten in dit tweedimensionale vlak, geef ze een kleur die correspondeert met het echte label en denk goed na over wat je ziet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_include = os.path.join('uitwerkingen', '4-nn_dimensiereductieresultaat.py') \n",
    "# %load $to_include"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Als je zin en tijd hebt:**\n",
    "\n",
    "Aangezien dit zo goed werkt, zou het dan ook werken om de plaatjes te reconstrueren met een bottleneck-laag van slechts twee neuronen? Probeer het eens!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_include = os.path.join('uitwerkingen', '4-nn_kleineencoder.py') \n",
    "# %load $to_include"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. Een denoising auto-encoder\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
